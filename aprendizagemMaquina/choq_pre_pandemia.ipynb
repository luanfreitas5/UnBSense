{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodo = 'pre-pandemia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importe as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3e410c3508beb0ff1c681badbacb4e0d32ceeb0a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors._classification import KNeighborsClassifier\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPClassifier\n",
    "from sklearn.linear_model._logistic import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.style.colors import resolve_colors\n",
    "from sklearn.model_selection._validation import validation_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from unidecode import unidecode\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "#import statsmodels.api as sm\n",
    "import platform \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strfdelta(tdelta, fmt=\"{hours}:{minutes}:{seconds}\"):\n",
    "    d = {\"days\": tdelta.days}\n",
    "    d[\"hours\"], rem = divmod(tdelta.seconds, 3600)\n",
    "    d[\"minutes\"], d[\"seconds\"] = divmod(rem, 60)\n",
    "    return fmt.format(**d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3fb5c911a3ab7c120bc3e0abe013e68174776a0"
   },
   "source": [
    "# Carregando Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f9e7bd1069bcc06d3ccb816ae5b6ba6d84041386"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volumeTweets_media</th>\n",
       "      <th>volumeTweets_variancia</th>\n",
       "      <th>volumeTweets_mediaMovelPonterada</th>\n",
       "      <th>volumeTweets_entropia</th>\n",
       "      <th>indiceInsonia_media</th>\n",
       "      <th>indiceInsonia_variancia</th>\n",
       "      <th>indiceInsonia_mediaMovelPonterada</th>\n",
       "      <th>indiceInsonia_entropia</th>\n",
       "      <th>pronome1Pessoa_media</th>\n",
       "      <th>pronome1Pessoa_variancia</th>\n",
       "      <th>...</th>\n",
       "      <th>links_entropia</th>\n",
       "      <th>midia_media</th>\n",
       "      <th>midia_variancia</th>\n",
       "      <th>midia_mediaMovelPonterada</th>\n",
       "      <th>midia_entropia</th>\n",
       "      <th>curtidas_media</th>\n",
       "      <th>curtidas_variancia</th>\n",
       "      <th>curtidas_mediaMovelPonterada</th>\n",
       "      <th>curtidas_entropia</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.232915</td>\n",
       "      <td>0.315346</td>\n",
       "      <td>1.605634</td>\n",
       "      <td>0.593198</td>\n",
       "      <td>0.086239</td>\n",
       "      <td>0.111151</td>\n",
       "      <td>0.578638</td>\n",
       "      <td>0.307958</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.022035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324054</td>\n",
       "      <td>0.006974</td>\n",
       "      <td>0.006925</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.041577</td>\n",
       "      <td>0.103208</td>\n",
       "      <td>0.179027</td>\n",
       "      <td>0.725352</td>\n",
       "      <td>0.320571</td>\n",
       "      <td>depressao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.510294</td>\n",
       "      <td>47.111659</td>\n",
       "      <td>17.270431</td>\n",
       "      <td>1.838695</td>\n",
       "      <td>0.620732</td>\n",
       "      <td>1.595054</td>\n",
       "      <td>4.288012</td>\n",
       "      <td>1.377158</td>\n",
       "      <td>0.483824</td>\n",
       "      <td>3.058562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048529</td>\n",
       "      <td>5.416763</td>\n",
       "      <td>7.237741</td>\n",
       "      <td>1.303106</td>\n",
       "      <td>depressao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356322</td>\n",
       "      <td>1.985678</td>\n",
       "      <td>2.425234</td>\n",
       "      <td>0.612006</td>\n",
       "      <td>0.146022</td>\n",
       "      <td>0.323983</td>\n",
       "      <td>1.003827</td>\n",
       "      <td>0.443252</td>\n",
       "      <td>0.112644</td>\n",
       "      <td>0.251679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.016263</td>\n",
       "      <td>controle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154891</td>\n",
       "      <td>0.891770</td>\n",
       "      <td>1.027701</td>\n",
       "      <td>0.334143</td>\n",
       "      <td>0.034081</td>\n",
       "      <td>0.039944</td>\n",
       "      <td>0.243190</td>\n",
       "      <td>0.160835</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.033288</td>\n",
       "      <td>0.891967</td>\n",
       "      <td>0.183438</td>\n",
       "      <td>controle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.116279</td>\n",
       "      <td>1931.916712</td>\n",
       "      <td>206.861111</td>\n",
       "      <td>2.842062</td>\n",
       "      <td>2.232019</td>\n",
       "      <td>15.639582</td>\n",
       "      <td>16.762029</td>\n",
       "      <td>2.242101</td>\n",
       "      <td>10.953488</td>\n",
       "      <td>269.858302</td>\n",
       "      <td>...</td>\n",
       "      <td>2.348284</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>1.176852</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>0.795581</td>\n",
       "      <td>4.023256</td>\n",
       "      <td>75.557599</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>1.857342</td>\n",
       "      <td>depressao</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   volumeTweets_media  volumeTweets_variancia  \\\n",
       "0            0.232915                0.315346   \n",
       "1            2.510294               47.111659   \n",
       "2            0.356322                1.985678   \n",
       "3            0.154891                0.891770   \n",
       "4           30.116279             1931.916712   \n",
       "\n",
       "   volumeTweets_mediaMovelPonterada  volumeTweets_entropia  \\\n",
       "0                          1.605634               0.593198   \n",
       "1                         17.270431               1.838695   \n",
       "2                          2.425234               0.612006   \n",
       "3                          1.027701               0.334143   \n",
       "4                        206.861111               2.842062   \n",
       "\n",
       "   indiceInsonia_media  indiceInsonia_variancia  \\\n",
       "0             0.086239                 0.111151   \n",
       "1             0.620732                 1.595054   \n",
       "2             0.146022                 0.323983   \n",
       "3             0.034081                 0.039944   \n",
       "4             2.232019                15.639582   \n",
       "\n",
       "   indiceInsonia_mediaMovelPonterada  indiceInsonia_entropia  \\\n",
       "0                           0.578638                0.307958   \n",
       "1                           4.288012                1.377158   \n",
       "2                           1.003827                0.443252   \n",
       "3                           0.243190                0.160835   \n",
       "4                          16.762029                2.242101   \n",
       "\n",
       "   pronome1Pessoa_media  pronome1Pessoa_variancia  ...  links_entropia  \\\n",
       "0              0.016736                  0.022035  ...        0.324054   \n",
       "1              0.483824                  3.058562  ...        0.159815   \n",
       "2              0.112644                  0.251679  ...        0.000000   \n",
       "3              0.008152                  0.013520  ...        0.000000   \n",
       "4             10.953488                269.858302  ...        2.348284   \n",
       "\n",
       "   midia_media  midia_variancia  midia_mediaMovelPonterada  midia_entropia  \\\n",
       "0     0.006974         0.006925                   0.049296        0.041577   \n",
       "1     0.000000         0.000000                   0.000000        0.000000   \n",
       "2     0.000000         0.000000                   0.000000        0.000000   \n",
       "3     0.000000         0.000000                   0.000000        0.000000   \n",
       "4     0.441860         1.176852                   3.388889        0.795581   \n",
       "\n",
       "   curtidas_media  curtidas_variancia  curtidas_mediaMovelPonterada  \\\n",
       "0        0.103208            0.179027                      0.725352   \n",
       "1        1.048529            5.416763                      7.237741   \n",
       "2        0.002299            0.002294                      0.002336   \n",
       "3        0.125000            1.033288                      0.891967   \n",
       "4        4.023256           75.557599                     25.500000   \n",
       "\n",
       "   curtidas_entropia     classe  \n",
       "0           0.320571  depressao  \n",
       "1           1.303106  depressao  \n",
       "2           0.016263   controle  \n",
       "3           0.183438   controle  \n",
       "4           1.857342  depressao  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'twitterbase_{periodo}.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2499434158d5acf41209b7225e3dcbcdbb32186e"
   },
   "source": [
    "# Codificando a variável categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "8bab65e1ac9710cbf92c126f78ffdb766cd76fd7"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['classe'] = label_encoder.fit_transform(df['classe']).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(X, y, train_ratio, val_ratio, test_ratio):\n",
    "    assert sum([train_ratio, test_ratio, val_ratio])==1.0, \"wrong given ratio, all ratios have to sum to 1.0\"\n",
    "    assert X.shape[0]==len(y), \"X and y shape mismatch\"\n",
    "\n",
    "    ind_train = int(round(X.shape[0]*train_ratio))\n",
    "    ind_test = int(round(X.shape[0]*(train_ratio+test_ratio)))\n",
    "\n",
    "    X_train = X[:ind_train]\n",
    "    X_test = X[ind_train:ind_test]\n",
    "    X_val = X[ind_test:]\n",
    "\n",
    "    y_train = y[:ind_train]\n",
    "    y_test = y[ind_train:ind_test]\n",
    "    y_val = y[ind_test:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def avaliacao(classificador, predicao, x, y, nome_classificador):\n",
    "    '''\n",
    "    Computa as mericas de avaliação dos clasficadores\n",
    "    '''\n",
    "        \n",
    "    precisao = precision_score(y, predicao)\n",
    "    revocacao = recall_score(y, predicao)\n",
    "    f1score = f1_score(y, predicao)\n",
    "    acuracia = accuracy_score(y, predicao)\n",
    "    return [precisao, revocacao, f1score, acuracia]\n",
    "    \n",
    "def modeloML(model, x, x_train, y_train, x_val, y_val, nome_classificador, atributo, i):\n",
    "    clear_output()\n",
    "    print(f'Experimento {i}')\n",
    "    print(nome_classificador)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predicao = cross_val_predict(model, x_val, y_val, cv=cv)\n",
    "    #predicao = GridSearchCV(model, {}, cv=cv, verbose=10, n_jobs=3).fit(x_train, y_train).predict(x_val)\n",
    "    #predicao = cross_val_predict(GridSearchCV(model, {}, cv=cv, verbose=10).fit(x_train, y_train).best_estimator_, x_val, y_val, cv=cv)\n",
    "    resultados = avaliacao(model, predicao, x_val, y_val, nome_classificador)\n",
    "    df_resultados.loc[nome_classificador][f'exp_basico_DeChoudhury_precisao'] = resultados[0]\n",
    "    df_resultados.loc[nome_classificador][f'exp_basico_DeChoudhury_recall'] = resultados[1]\n",
    "    df_resultados.loc[nome_classificador][f'exp_basico_DeChoudhury_f1_score'] = resultados[2]\n",
    "    df_resultados.loc[nome_classificador][f'exp_basico_DeChoudhury_acuracia'] = resultados[3]\n",
    "    \n",
    "    #plotarMatrizConfusao(y_val, predicao, nome_classificador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "zscore = StandardScaler()\n",
    "\n",
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'df_resultados_{periodo}_f1Score.xlsx'):\n",
    "    metricas = ['acuracia', 'precisao', 'recall', 'f1_score']\n",
    "    colunas = [f'exp_basico_DeChoudhury_{i}' for i in metricas]\n",
    "    modelos = ['Regressão Logistica (Baseline)', 'Análise Discriminante Linear', 'KNN', 'Naive Bayes',\n",
    "               'Árvore de Decisão', 'Floresta Randômica', 'Gradient Boosting', 'Perceptron Multicamadas', 'SVM',\n",
    "               'Bagging', 'Boosting', 'Votação Hard', 'Votação Soft'\n",
    "              ]\n",
    "    df_resultados = pd.DataFrame(columns=colunas, index=modelos)\n",
    "    df_resultados.index.name = \"Modelo\"\n",
    "else:\n",
    "    df_resultados = pd.read_excel(f'df_resultados_{periodo}_f1Score.xlsx', index_col=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_basico_DeChoudhury_acuracia</th>\n",
       "      <th>exp_basico_DeChoudhury_precisao</th>\n",
       "      <th>exp_basico_DeChoudhury_recall</th>\n",
       "      <th>exp_basico_DeChoudhury_f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Regressão Logistica (Baseline)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Análise Discriminante Linear</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Árvore de Decisão</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floresta Randômica</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron Multicamadas</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boosting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Votação Hard</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Votação Soft</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               exp_basico_DeChoudhury_acuracia  \\\n",
       "Modelo                                                           \n",
       "Regressão Logistica (Baseline)                             NaN   \n",
       "Análise Discriminante Linear                               NaN   \n",
       "KNN                                                        NaN   \n",
       "Naive Bayes                                                NaN   \n",
       "Árvore de Decisão                                          NaN   \n",
       "Floresta Randômica                                         NaN   \n",
       "Gradient Boosting                                          NaN   \n",
       "Perceptron Multicamadas                                    NaN   \n",
       "SVM                                                        NaN   \n",
       "Bagging                                                    NaN   \n",
       "Boosting                                                   NaN   \n",
       "Votação Hard                                               NaN   \n",
       "Votação Soft                                               NaN   \n",
       "\n",
       "                               exp_basico_DeChoudhury_precisao  \\\n",
       "Modelo                                                           \n",
       "Regressão Logistica (Baseline)                             NaN   \n",
       "Análise Discriminante Linear                               NaN   \n",
       "KNN                                                        NaN   \n",
       "Naive Bayes                                                NaN   \n",
       "Árvore de Decisão                                          NaN   \n",
       "Floresta Randômica                                         NaN   \n",
       "Gradient Boosting                                          NaN   \n",
       "Perceptron Multicamadas                                    NaN   \n",
       "SVM                                                        NaN   \n",
       "Bagging                                                    NaN   \n",
       "Boosting                                                   NaN   \n",
       "Votação Hard                                               NaN   \n",
       "Votação Soft                                               NaN   \n",
       "\n",
       "                               exp_basico_DeChoudhury_recall  \\\n",
       "Modelo                                                         \n",
       "Regressão Logistica (Baseline)                           NaN   \n",
       "Análise Discriminante Linear                             NaN   \n",
       "KNN                                                      NaN   \n",
       "Naive Bayes                                              NaN   \n",
       "Árvore de Decisão                                        NaN   \n",
       "Floresta Randômica                                       NaN   \n",
       "Gradient Boosting                                        NaN   \n",
       "Perceptron Multicamadas                                  NaN   \n",
       "SVM                                                      NaN   \n",
       "Bagging                                                  NaN   \n",
       "Boosting                                                 NaN   \n",
       "Votação Hard                                             NaN   \n",
       "Votação Soft                                             NaN   \n",
       "\n",
       "                               exp_basico_DeChoudhury_f1_score  \n",
       "Modelo                                                          \n",
       "Regressão Logistica (Baseline)                             NaN  \n",
       "Análise Discriminante Linear                               NaN  \n",
       "KNN                                                        NaN  \n",
       "Naive Bayes                                                NaN  \n",
       "Árvore de Decisão                                          NaN  \n",
       "Floresta Randômica                                         NaN  \n",
       "Gradient Boosting                                          NaN  \n",
       "Perceptron Multicamadas                                    NaN  \n",
       "SVM                                                        NaN  \n",
       "Bagging                                                    NaN  \n",
       "Boosting                                                   NaN  \n",
       "Votação Hard                                               NaN  \n",
       "Votação Soft                                               NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executar(x, y, atributo, i):\n",
    "\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = train_val_test(x, y, train_ratio, validation_ratio, test_ratio)\n",
    "\n",
    "    baseline = LogisticRegression(random_state=42, C=1e-06, n_jobs=-1)\n",
    "    analiseDiscriminanteLinear = LinearDiscriminantAnalysis(shrinkage=0, solver='eigen', tol=1e-6)\n",
    "    knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-1)\n",
    "    naiveBayes = BernoulliNB(alpha=1e-09)\n",
    "\n",
    "    arvoreDecisao = DecisionTreeClassifier(random_state=42, max_depth=9)\n",
    "    florestaRandomica = RandomForestClassifier(random_state=42, max_depth=10, n_estimators=90, verbose=10, n_jobs=-1)\n",
    "    gradientBoosting = GradientBoostingClassifier(random_state=42, n_estimators=100, verbose=10)\n",
    "    perceptronMulticamadas = MLPClassifier(random_state=42, hidden_layer_sizes=(10,30,10), max_iter=100, verbose=10)\n",
    "    svm = LinearSVC(random_state=42, C=1, max_iter=1000, verbose=10)\n",
    "    \n",
    "    modeloML(baseline, x, x_train, y_train, x_val, y_val, 'Regressão Logistica (Baseline)', atributo, i)\n",
    "    modeloML(analiseDiscriminanteLinear, x, x_train, y_train, x_val, y_val, 'Análise Discriminante Linear', atributo, i)\n",
    "    modeloML(naiveBayes, x, x_train, y_train, x_val, y_val, 'Naive Bayes', atributo, i)\n",
    "    modeloML(arvoreDecisao, x, x_train, y_train, x_val, y_val, 'Árvore de Decisão', atributo, i)\n",
    "    modeloML(knn, x, x_train, y_train, x_val, y_val, 'KNN', atributo, i)\n",
    "    modeloML(svm, x, x_train, y_train, x_val, y_val, 'SVM', atributo, i)\n",
    "    modeloML(florestaRandomica, x, x_train, y_train, x_val, y_val, 'Floresta Randômica', atributo, i)\n",
    "    modeloML(perceptronMulticamadas, x, x_train, y_train, x_val, y_val, 'Perceptron Multicamadas', atributo, i)\n",
    "    modeloML(gradientBoosting, x, x_train, y_train, x_val, y_val, 'Gradient Boosting', atributo, i)\n",
    "    \n",
    "    florestaRandomica.set_params(verbose=0)\n",
    "    gradientBoosting.set_params(verbose=0)\n",
    "    perceptronMulticamadas.set_params(verbose=0)\n",
    "    svm.set_params(verbose=0)\n",
    "\n",
    "    listaEstimators = [('Análise Discriminante Linear', analiseDiscriminanteLinear), \n",
    "                       ('Naive Bayes', naiveBayes), \n",
    "                       ('KNN', knn), \n",
    "                       ('Árvore de Decisão', arvoreDecisao), \n",
    "                       ('Floresta Randômica', florestaRandomica), \n",
    "                       ('Perceptron Multicamadas', perceptronMulticamadas),\n",
    "                       ('Gradient Boosting', gradientBoosting),\n",
    "                       ('SVM', svm),\n",
    "                      ]\n",
    "\n",
    "\n",
    "    bagging = BaggingClassifier(base_estimator=perceptronMulticamadas, n_estimators=10, random_state=42, verbose=10, n_jobs=-1)\n",
    "    boosting = AdaBoostClassifier(base_estimator=florestaRandomica, n_estimators=10, random_state=42) \n",
    "    votingHard = VotingClassifier(voting='hard', estimators=listaEstimators, verbose=10, n_jobs=-1)\n",
    "    votingSoft = VotingClassifier(voting='soft', estimators=listaEstimators[:-1], verbose=10, n_jobs=-1)\n",
    "\n",
    "    modeloML(votingHard, x, x_train, y_train, x_val, y_val, 'Votação Hard', atributo, i)\n",
    "    modeloML(votingSoft, x, x_train, y_train, x_val, y_val, 'Votação Soft', atributo, i)\n",
    "    modeloML(bagging, x, x_train, y_train, x_val, y_val, 'Bagging', atributo, i)\n",
    "    modeloML(boosting, x, x_train, y_train, x_val, y_val, 'Boosting', atributo, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "choq = ['volumeTweets_media', 'volumeTweets_mediaMovelPonterada', \n",
    "        'indiceInsonia_variancia', 'indiceInsonia_mediaMovelPonterada',\n",
    "        'pronome1Pessoa_media', 'pronome1Pessoa_variancia', \n",
    "        'pronome2Pessoa_mediaMovelPonterada', 'pronome2Pessoa_entropia',\n",
    "        'pronome3Pessoa_media', 'pronome3Pessoa_mediaMovelPonterada',\n",
    "        'valencia_mediaMovelPonterada', 'valencia_entropia', \n",
    "        'ativacao_mediaMovelPonterada', 'ativacao_entropia', \n",
    "        'termosDepressivos_variancia', 'termosDepressivos_mediaMovelPonterada',\n",
    "        'grafoSocial_variancia', 'grafoSocial_mediaMovelPonterada',\n",
    "        'medicamentosAntiDepressivo_media', 'medicamentosAntiDepressivo_mediaMovelPonterada']\n",
    "caracteresOrientais = ['caracteresOrientais_variancia', 'caracteresOrientais_mediaMovelPonterada']\n",
    "emojis = ['emojis_variancia', 'emojis_entropia']\n",
    "links = ['links_mediaMovelPonterada', 'links_entropia']\n",
    "midia = ['midia_variancia', 'midia_mediaMovelPonterada']\n",
    "curtidas = ['curtidas_media', 'curtidas_mediaMovelPonterada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento 1\n",
      "Boosting\n"
     ]
    }
   ],
   "source": [
    "exp = choq\n",
    "x = df[exp].copy()\n",
    "y = df['classe'].copy()\n",
    "x_new = zscore.fit_transform(x)\n",
    "executar(x_new, y, '', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4c252\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4c252_level0_col0\" class=\"col_heading level0 col0\" >exp_basico_DeChoudhury_acuracia</th>\n",
       "      <th id=\"T_4c252_level0_col1\" class=\"col_heading level0 col1\" >exp_basico_DeChoudhury_precisao</th>\n",
       "      <th id=\"T_4c252_level0_col2\" class=\"col_heading level0 col2\" >exp_basico_DeChoudhury_recall</th>\n",
       "      <th id=\"T_4c252_level0_col3\" class=\"col_heading level0 col3\" >exp_basico_DeChoudhury_f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Modelo</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row0\" class=\"row_heading level0 row0\" >Gradient Boosting</th>\n",
       "      <td id=\"T_4c252_row0_col0\" class=\"data row0 col0\" >77.2</td>\n",
       "      <td id=\"T_4c252_row0_col1\" class=\"data row0 col1\" >75.5</td>\n",
       "      <td id=\"T_4c252_row0_col2\" class=\"data row0 col2\" >90.1</td>\n",
       "      <td id=\"T_4c252_row0_col3\" class=\"data row0 col3\" >82.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row1\" class=\"row_heading level0 row1\" >Boosting</th>\n",
       "      <td id=\"T_4c252_row1_col0\" class=\"data row1 col0\" >77.3</td>\n",
       "      <td id=\"T_4c252_row1_col1\" class=\"data row1 col1\" >76.2</td>\n",
       "      <td id=\"T_4c252_row1_col2\" class=\"data row1 col2\" >88.8</td>\n",
       "      <td id=\"T_4c252_row1_col3\" class=\"data row1 col3\" >82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row2\" class=\"row_heading level0 row2\" >Floresta Randômica</th>\n",
       "      <td id=\"T_4c252_row2_col0\" class=\"data row2 col0\" >76.6</td>\n",
       "      <td id=\"T_4c252_row2_col1\" class=\"data row2 col1\" >74.8</td>\n",
       "      <td id=\"T_4c252_row2_col2\" class=\"data row2 col2\" >90.1</td>\n",
       "      <td id=\"T_4c252_row2_col3\" class=\"data row2 col3\" >81.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row3\" class=\"row_heading level0 row3\" >Bagging</th>\n",
       "      <td id=\"T_4c252_row3_col0\" class=\"data row3 col0\" >76.5</td>\n",
       "      <td id=\"T_4c252_row3_col1\" class=\"data row3 col1\" >75.9</td>\n",
       "      <td id=\"T_4c252_row3_col2\" class=\"data row3 col2\" >87.6</td>\n",
       "      <td id=\"T_4c252_row3_col3\" class=\"data row3 col3\" >81.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row4\" class=\"row_heading level0 row4\" >Perceptron Multicamadas</th>\n",
       "      <td id=\"T_4c252_row4_col0\" class=\"data row4 col0\" >76.1</td>\n",
       "      <td id=\"T_4c252_row4_col1\" class=\"data row4 col1\" >75.5</td>\n",
       "      <td id=\"T_4c252_row4_col2\" class=\"data row4 col2\" >87.4</td>\n",
       "      <td id=\"T_4c252_row4_col3\" class=\"data row4 col3\" >81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row5\" class=\"row_heading level0 row5\" >Votação Soft</th>\n",
       "      <td id=\"T_4c252_row5_col0\" class=\"data row5 col0\" >76.1</td>\n",
       "      <td id=\"T_4c252_row5_col1\" class=\"data row5 col1\" >75.8</td>\n",
       "      <td id=\"T_4c252_row5_col2\" class=\"data row5 col2\" >86.7</td>\n",
       "      <td id=\"T_4c252_row5_col3\" class=\"data row5 col3\" >80.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row6\" class=\"row_heading level0 row6\" >Votação Hard</th>\n",
       "      <td id=\"T_4c252_row6_col0\" class=\"data row6 col0\" >75.7</td>\n",
       "      <td id=\"T_4c252_row6_col1\" class=\"data row6 col1\" >75.7</td>\n",
       "      <td id=\"T_4c252_row6_col2\" class=\"data row6 col2\" >85.8</td>\n",
       "      <td id=\"T_4c252_row6_col3\" class=\"data row6 col3\" >80.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row7\" class=\"row_heading level0 row7\" >Árvore de Decisão</th>\n",
       "      <td id=\"T_4c252_row7_col0\" class=\"data row7 col0\" >74.7</td>\n",
       "      <td id=\"T_4c252_row7_col1\" class=\"data row7 col1\" >74.1</td>\n",
       "      <td id=\"T_4c252_row7_col2\" class=\"data row7 col2\" >86.9</td>\n",
       "      <td id=\"T_4c252_row7_col3\" class=\"data row7 col3\" >80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row8\" class=\"row_heading level0 row8\" >KNN</th>\n",
       "      <td id=\"T_4c252_row8_col0\" class=\"data row8 col0\" >70.5</td>\n",
       "      <td id=\"T_4c252_row8_col1\" class=\"data row8 col1\" >70.6</td>\n",
       "      <td id=\"T_4c252_row8_col2\" class=\"data row8 col2\" >84.7</td>\n",
       "      <td id=\"T_4c252_row8_col3\" class=\"data row8 col3\" >77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row9\" class=\"row_heading level0 row9\" >SVM</th>\n",
       "      <td id=\"T_4c252_row9_col0\" class=\"data row9 col0\" >68.3</td>\n",
       "      <td id=\"T_4c252_row9_col1\" class=\"data row9 col1\" >68.9</td>\n",
       "      <td id=\"T_4c252_row9_col2\" class=\"data row9 col2\" >83.1</td>\n",
       "      <td id=\"T_4c252_row9_col3\" class=\"data row9 col3\" >75.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row10\" class=\"row_heading level0 row10\" >Análise Discriminante Linear</th>\n",
       "      <td id=\"T_4c252_row10_col0\" class=\"data row10 col0\" >66.9</td>\n",
       "      <td id=\"T_4c252_row10_col1\" class=\"data row10 col1\" >67.3</td>\n",
       "      <td id=\"T_4c252_row10_col2\" class=\"data row10 col2\" >84.2</td>\n",
       "      <td id=\"T_4c252_row10_col3\" class=\"data row10 col3\" >74.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row11\" class=\"row_heading level0 row11\" >Regressão Logistica (Baseline)</th>\n",
       "      <td id=\"T_4c252_row11_col0\" class=\"data row11 col0\" >58.3</td>\n",
       "      <td id=\"T_4c252_row11_col1\" class=\"data row11 col1\" >58.3</td>\n",
       "      <td id=\"T_4c252_row11_col2\" class=\"data row11 col2\" >100.0</td>\n",
       "      <td id=\"T_4c252_row11_col3\" class=\"data row11 col3\" >73.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c252_level0_row12\" class=\"row_heading level0 row12\" >Naive Bayes</th>\n",
       "      <td id=\"T_4c252_row12_col0\" class=\"data row12 col0\" >59.1</td>\n",
       "      <td id=\"T_4c252_row12_col1\" class=\"data row12 col1\" >71.3</td>\n",
       "      <td id=\"T_4c252_row12_col2\" class=\"data row12 col2\" >49.9</td>\n",
       "      <td id=\"T_4c252_row12_col3\" class=\"data row12 col3\" >58.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2164ec95b70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultados_round = df_resultados.sort_values(by='exp_basico_DeChoudhury_f1_score',ascending=False).astype('float64').round(3) * 100\n",
    "df_resultados_round.style.set_precision(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados_round.to_excel(f'df_resultados_{periodo}_f1Score.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
